# Specification: Standardized Evaluation Node Framework

## 1. Overview

This document outlines the specification for a standardized evaluation framework within the `ProposalGenerationGraph` system. This framework will provide a consistent approach to evaluating various types of content (research results, solution analysis, connection pairs, and proposal sections) with standardized patterns for state management, Human-in-the-Loop (HITL) integration, and error handling.

## 2. System Context

The evaluation framework is a core component of the proposal generation system that:

- Provides quality control checkpoints after content generation
- Integrates with LangGraph.js for state management and workflow control
- Supports the HITL workflow through standard interrupt points
- Works within the existing state structure defined by `OverallProposalState`
- Operates under the supervision of the Orchestrator Service

## 3. Core Framework Components

### 3.1. Evaluation Node Factory

A reusable factory function that generates specialized evaluation nodes:

```typescript
/**
 * Creates a standardized evaluation node for a specific content type
 * @param options Configuration for the evaluation node
 * @returns A node function compatible with the LangGraph StateGraph
 */
function createEvaluationNode(
  options: EvaluationNodeOptions
): EvaluationNodeFunction {
  // Implementation details
}

type EvaluationNodeFunction = (
  state: OverallProposalState
) => Promise<Partial<OverallProposalState>>;

interface EvaluationNodeOptions {
  contentType: string; // Type of content being evaluated (e.g., "research", "solution")
  contentExtractor: ContentExtractor; // Function to extract content from state
  criteriaPath: string; // Path to criteria configuration JSON
  evaluationPrompt?: string; // Optional custom prompt override
  resultField: string; // State field to store evaluation results
  statusField: string; // State field to update status
  passingThreshold?: number; // Threshold score to pass evaluation (default: 0.7)
  modelName?: string; // LLM model to use (default from config)
  customValidator?: ResultValidator; // Optional custom validation logic
}
```

### 3.2. Evaluation Result Interface

Standardized structure for all evaluation results:

```typescript
interface EvaluationResult {
  passed: boolean; // Overall pass/fail determination
  timestamp: string; // When evaluation was performed
  evaluator: "ai" | "human" | string; // Source of evaluation
  overallScore: number; // Combined weighted score (0.0-1.0)
  scores: {
    // Individual criteria scores
    [criterionId: string]: number; // 0.0-1.0 range for each criterion
  };
  strengths: string[]; // Identified strengths
  weaknesses: string[]; // Areas for improvement
  suggestions: string[]; // Specific improvement recommendations
  feedback: string; // Overall assessment commentary
  rawResponse?: any; // Original evaluation response (for debugging)
}
```

### 3.3. Criteria Configuration Structure

JSON-based configuration for evaluation criteria:

```typescript
interface EvaluationCriteria {
  id: string; // Unique identifier for the configuration
  name: string; // Human-readable name
  version: string; // Version number for tracking changes
  criteria: Array<{
    id: string; // Unique identifier for the criterion
    name: string; // Human-readable name
    description: string; // Detailed explanation
    weight: number; // Relative importance (0.0-1.0)
    isCritical: boolean; // If true, failing this fails overall
    passingThreshold: number; // Minimum score to pass (0.0-1.0)
    scoringGuidelines: {
      // Guidelines for scoring
      excellent: string; // Description of score 0.9-1.0
      good: string; // Description of score 0.7-0.89
      adequate: string; // Description of score 0.5-0.69
      poor: string; // Description of score 0.3-0.49
      inadequate: string; // Description of score 0.0-0.29
    };
  }>;
  passingThreshold: number; // Overall threshold to pass (0.0-1.0)
}
```

## 4. Core Processing Logic

### 4.1. Standard Node Execution Flow

Every evaluation node generated by the factory follows this standard flow:

1. **Input Validation:**

   - Verify existence and format of content to evaluate
   - Check state readiness for evaluation
   - Return appropriate error state if validation fails

2. **Status Update:**

   - Set content-specific status to `'evaluating'`

3. **Criteria Loading:**

   - Load and validate evaluation criteria from specified path
   - Apply default criteria if specific criteria unavailable

4. **Prompt Construction:**

   - Create evaluation prompt with content and criteria
   - Include examples and scoring guidelines

5. **Agent/LLM Invocation:**

   - Call evaluation agent with constructed prompt
   - Apply timeout protection (60 seconds default)

6. **Response Processing:**

   - Parse and validate structured evaluation results
   - Calculate overall score based on criteria weights
   - Determine pass/fail status based on thresholds

7. **State Updates:**

   - Store evaluation results in appropriate state field
   - Update status to `'awaiting_review'`
   - Set interrupt flag for HITL review
   - Add informational message to state.messages

8. **Return Updated State:**
   - Return partial state with all updates

### 4.2. Human-in-the-Loop Integration

Every evaluation node automatically integrates with HITL workflow:

1. **Interrupt Signaling:**

   - Set `isInterrupted: true` in returned state
   - Include appropriate metadata for UI presentation:
     ```typescript
     interruptMetadata: {
       type: 'evaluation_review',
       contentType: options.contentType,
       evaluation: evaluationResult,
       actions: ['approve', 'revise', 'edit'],
       title: `${humanReadableContentType} Evaluation Review`,
       description: `Review the evaluation of the ${humanReadableContentType}.`
     }
     ```

2. **Resume Processing:**
   - Orchestrator handles user feedback when workflow resumes
   - Based on user action:
     - `approve`: Continue to next node, status → `'approved'`
     - `revise`: Return to generation node, status → `'revision_requested'`
     - `edit`: Apply edits, status → `'edited'`, mark dependents as `'stale'`

## 5. State Management

### 5.1. State Fields Updated

Each evaluation node updates these state fields:

```typescript
{
  // Content-specific evaluation result
  [options.resultField]: EvaluationResult,

  // Content-specific status
  [options.statusField]: 'evaluating' | 'awaiting_review' | 'approved' | 'revision_requested' | 'edited' | 'error',

  // HITL interrupt flag
  isInterrupted: boolean,

  // HITL metadata for UI
  interruptMetadata?: InterruptMetadata,

  // Updated messages array
  messages: [...previousMessages, newMessage],

  // Error information (only on failure)
  errors?: [...previousErrors, newError]
}
```

### 5.2. Status Transitions

Each evaluation node manages these status transitions:

1. **Pre-evaluation**:

   - Initial: Content-specific status typically `'completed'` (from generation node)
   - Update: Set to `'evaluating'` during processing

2. **Post-evaluation**:

   - Update: Set to `'awaiting_review'` when ready for HITL

3. **Post-HITL** (managed by Orchestrator):

   - Update: Set to `'approved'`, `'revision_requested'`, or `'edited'` based on user action

4. **Error**:
   - Update: Set to `'error'` if processing fails

## 6. Error Handling

The framework provides standardized error handling for:

1. **Input Validation Errors:**

   - Missing required content
   - Malformed content structure
   - Invalid state for evaluation

2. **LLM/Agent Errors:**

   - Timeouts (using 60-second default protection)
   - API rate limits
   - Service unavailability
   - Content policy violations

3. **Processing Errors:**
   - JSON parsing failures
   - Schema validation errors
   - Scoring calculation errors

All errors follow this pattern:

- Set appropriate error status
- Add detailed error message to state.errors
- Include technical details for debugging
- Add user-friendly message to state.messages

## 7. Configuration System

### 7.1. Criteria Configuration Files

Stored as JSON files in a standardized location:

```
/config/evaluation/criteria/{contentType}.json
```

Example paths:

- `/config/evaluation/criteria/research.json`
- `/config/evaluation/criteria/solution.json`
- `/config/evaluation/criteria/connection_pairs.json`
- `/config/evaluation/criteria/problem_statement.json`

### 7.2. Evaluation Prompts

Stored as template files:

```
/prompts/evaluation/{contentType}EvaluationPrompt.js
```

Example paths:

- `/prompts/evaluation/researchEvaluationPrompt.js`
- `/prompts/evaluation/solutionEvaluationPrompt.js`

## 8. Integration Requirements

### 8.1. Graph Integration

Evaluation nodes should be added to the graph with:

```typescript
// Add the node to the graph
graph.addNode(
  `evaluate${ContentType}`,
  createEvaluationNode({
    contentType: contentTypeId,
    contentExtractor: extractorFunction,
    criteriaPath: `${contentType}.json`,
    resultField: `${contentType}Evaluation`,
    statusField: `${contentType}Status`,
  })
);

// Connect with appropriate edges
graph.addEdge(contentTypeNode, `evaluate${ContentType}`);

// Add conditional routing based on evaluation and HITL
graph.addConditionalEdges(`evaluate${ContentType}`, routeAfterEvaluation, {
  continue: nextStepNode,
  revise: contentTypeNode,
});
```

### 8.2. HITL Configuration

All evaluation nodes should be registered as interrupt points:

```typescript
// Configure interrupt points
graph.compiler.interruptAfter([
  "evaluateResearch",
  "evaluateSolution",
  "evaluateConnections",
  // ... other evaluation nodes
]);
```

### 8.3. Orchestrator Integration

The Orchestrator Service must be updated to:

1. Handle evaluation-specific interrupt data
2. Process user feedback for evaluations
3. Apply appropriate state transitions based on user actions
4. Manage dependency tracking when content is edited after evaluation

## 9. Success Criteria

The implementation is considered successful when:

1. **Factory Implementation:**

   - The `createEvaluationNode` factory function is implemented and testable
   - Configuration options properly customize node behavior
   - Generated nodes follow the standardized processing flow

2. **Result Interface:**

   - The `EvaluationResult` interface is implemented
   - Appropriate Zod schema validation is provided
   - Helper functions for scoring calculations are implemented

3. **Criteria Configuration:**

   - JSON schema for criteria is defined
   - Loading and validation mechanisms are implemented
   - Example configuration files for key content types are provided

4. **HITL Integration:**

   - Interrupt metadata includes evaluation results
   - UI hooks for review workflow are specified
   - State transitions after HITL are properly defined

5. **Specific Implementations:**

   - Evaluation nodes for research, solution, connections, and key sections are created
   - Each node has appropriate tests
   - Each node integrates with the graph correctly

6. **Documentation:**
   - Architecture documentation updated to reflect evaluation patterns
   - Configuration guide for creating custom criteria
   - Usage examples for all components

## 10. Implementation Timeline

1. **Phase 1 - Core Framework:**

   - Implement `EvaluationResult` interface and validation
   - Create criteria configuration system
   - Implement factory function core

2. **Phase 2 - Existing Node Migration:**

   - Refactor existing evaluation nodes to use the framework
   - Update tests to verify consistency
   - Validate HITL integration

3. **Phase 3 - New Node Implementation:**

   - Create evaluation nodes for remaining content types
   - Implement comprehensive testing
   - Update graph definition with new nodes

4. **Phase 4 - Documentation & Integration:**
   - Create detailed documentation
   - Complete Orchestrator integration
   - Validate full system workflow

## 11. Appendix: Integration with OverallProposalState

The evaluation framework aligns with the existing `OverallProposalState` by using consistent field naming and status values:

```typescript
// Existing fields that will be used by evaluation nodes:
interface OverallProposalState {
  // Research evaluation
  researchResults?: Record<string, any>;
  researchStatus: ProcessingStatus;
  researchEvaluation?: EvaluationResult | null;

  // Solution evaluation
  solutionSoughtResults?: Record<string, any>;
  solutionSoughtStatus: ProcessingStatus;
  solutionSoughtEvaluation?: EvaluationResult | null;

  // Connection evaluation
  connectionPairs?: any[];
  connectionPairsStatus: ProcessingStatus;
  connectionPairsEvaluation?: EvaluationResult | null;

  // Section evaluations
  sections: { [sectionId: string]: SectionData | undefined };

  // HITL infrastructure
  isInterrupted?: boolean;
  interruptMetadata?: any;

  // Communication and errors
  messages: BaseMessage[];
  errors: string[];
}
```

## 12. Appendix: Compatibility with Existing Nodes

The framework is designed to be compatible with existing nodes like `evaluateConnectionsNode`, providing a path for refactoring without breaking changes:

1. **Migration Approach:**

   - Create new factory-based implementation
   - Test compatibility with existing node inputs/outputs
   - Replace implementation while maintaining same interface
   - Update tests to verify consistent behavior

2. **Future Node Creation:**
   - All new evaluation nodes should use the factory
   - Maintain consistent patterns and interfaces
   - Extend the framework for specialized needs rather than creating exceptions
